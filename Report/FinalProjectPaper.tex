\documentclass[conference]{IEEEtran}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{cite}
\usepackage{caption}
\usepackage{float}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{algpseudocode}
\usepackage[most]{tcolorbox}
\usepackage{algorithm}
\usepackage{hyperref}
\usepackage{siunitx}


\tcbset{
  myalgo/.style={
    enhanced,
    colback=white,
    colframe=black,
    width=\columnwidth,
    boxrule=0.5pt,
    arc=2pt,
    left=2pt,
    right=2pt,
    top=2pt,
    bottom=2pt,
    before skip=0pt, 
    after skip=12pt,
  }
}

\title{ XOR Cache Compression}
\author{
\IEEEauthorblockN{
Imran Aliji,
Connor Antony,
Lila Craveiro,
Zannatun Naim Sristy}
\IEEEauthorblockA{
Department of Computer Science and Engineering, University of Minnesota\\
Email: \{aliji001, anton386, crave112, srist001\}@umn.edu}
}



\begin{document}
\maketitle

\section{Abstract}
Cache compression allows for the condensing of larger chunks of data into smaller chunks for storage. This method improves cache performance by attaining the benefits that come with increasing cache size without any of the latency concerns that come with it. One such approach to cache compression is by leveraging data redundancy across and within cache lines. The XOR Cache is a method that does just that. The XOR Cache conducts a series of XOR operations across and within cachelines in order to compress data, storing just the XORed line. XOR Cache is a structure that, in even naive implementations, can reduce entropy, and has potential for further benefits when used alongside other compression schemes. The XOR cache is able to return notable compression of memory that matches if not surpasses other compression schemes across numerous benchmarks, as well as doing so latency comparable to that of cache with no compression scheme.

\section{Introduction}
At its core, computing is the transfer of information and as time goes on, the amount of data we collect, process, and store grows. This presents a key challenge of how to utilize the resources you have as efficiently as possible so that you consume less power. This problem compounds as we take a look at the world of computing and how a few speed-ups on some algorithms have saved billions of dollars in energy use worldwide. One method that helps our compute performance is compression. The goal of compression is to take a larger chunk of data and condense them to take up less space. More importantly, we can move this information faster when it takes up less space because it takes up less bandwidth. Today's processors have dedicated space on their die built for fast access SRAM for caching purposes. The CPU cache is one of its most powerful tools for quickly accessing local memory, much faster than typical RAM accesses. However, a processor's die can only fit so many components, making the placement of each unit compromise the compute efficiency in some way. Knowing this, cache compression has been an avenue through which we can squeeze more power out of processors while utilizing fewer resources. 

There is a vast amount of scholarship on cache compression schemes\cite{2DCC,Base-delta-immediate_compression,Base-victim_compression,BCD_deduplication,Bunker_Cache,Dictionary_sharing,Doppelganger,Ecco,Exploiting_Inter-block_Entropy,Frequent_Pattern_Compression,HyComp,Last-level_cache_deduplication,MORC,SC2,Thesaurus,Touche}. One such method of cache compression is the Xor Cache. \cite{The_XOR_Cache}. The XOR cache leverages redundancy across cache lines. When a line of data that already exits in the cache tries to be stored, this compression scheme will instead XOR this line with the existing line at a lower level in the cache. On access, another XOR operation is performed on the same line in order to decompress it.

The XOR Cache is a design that not only has the benefits of saving storage and reducing entropy, but it is also quite simple to understand. It is for these reasons that we chose to do a replicative study on the XOR Cache as a method of cache compression. In our investigation, we implement the XOR cache structure and test its ability to compress data effectively on a number of benchmarks.

\section{Related Works}
Cache compression is a prolific branch of research in the pursuit of improving cache performance. Cache compression allows for the benefits that come with increasing cache size without any of the latency or other challenges that arise from large cache size \cite{Understanding_Cache_Compression}. This method works by compressing lines upon insertion and decompressing them upon access. While the higher level idea behind cache compression is the same, methods of cache compression defer by their chosen cache compaction scheme. This scheme defines the decisions made by the technique that explain why, when, how, and where compression is being applied and the data is placed after compression \cite{Understanding_Cache_Compression}. 

There are a number of factors that go into a method's cache compaction scheme. These factors define the ways in which cache compression methods address the handling of data and mapping of compressed data. Some factors include expansion, mapping, and tag optimization. Carvalho and Seznec \cite{Understanding_Cache_Compression} describe these concepts as follows: 

\textit{Expansion}: actions taken when a compressed block experiences a fat write

\textit{Mapping}: mapping between tag and data storages

\textit{Tag Optimization}: optimization quality of tag representation

In addition to these defining features, another significant factor of a cache's compaction scheme is its \textit{Granularity}. Granularity defines where deduplication occurs: between values in a line, or between the lines themselves. There are benefits to both types of compression, but they also need not be done in isolation. Some compression methods, like XOR Cache \cite{The_XOR_Cache}, make use of both methods, taking advantage of the benefits of both schemes.

\subsection{Intra-Line Compression}
Intra-line cache compression strategies primarily concentrate on reducing individual cache blocks by avoiding repetitive patterns or slight value variances inside a single line in order to get around the limited Last-Level Cache (LLC) capacity. Traditional data cache compressors such as Frequent Pattern Compression (FPC) \cite{Frequent_Pattern_Compression} and Base-Delta-Immediate (BDI) \cite{Base-delta-immediate_compression} use the idea that the data values stored within the line often exhibit low value differences. They use a common base value plus an array of relative differences to compress the cache line, whose combined size is much smaller than the original cache line. Hybrid intra-line compression approaches, on the other hand, such as HyComp \cite{HyComp} further exploit value locality of multiple data types by selecting among several compression algorithms (e.g., BDI \cite{Base-delta-immediate_compression}, Huffman-based schemes \cite{SC2}) based on detected semantic characteristics within the line. Even recent domain-specific schemes such as Ecco \cite{Ecco} build upon traditional Huffman-based schemes \cite{SC2} with augmentation of group-wise and non-uniform quantization with pre-defined shared k-means patterns to exploit the inherent entropy characteristics of LLM cache data. In contrast, Base-Victim Compression (BVC) \cite{Base-victim_compression} uses two tags: a base line - the line that would exist without compression - and a victim line - the line kept in place if able to be compressed to fit an existing base line - to achieve the same hit rate of an uncompressed cache.

Even though intra-line approaches greatly improve the LLC capacity, they are unable to take advantage of structural similarities across various blocks since they operate independently on each cache line. This motivates inter-line techniques and layered hybrid approaches which build on traditional intra-line compression while additionally exploiting cross-line structural correlation to further expand usable capacity.


\subsection{Inter-Line Compression}
While intra-line cache compression reduces data size by exploiting internal line patterns, inter-line compression leverages similarity across multiple cache lines to further reduce redundancy and increase usable cache space. For example, Deduplication \cite{Last-level_cache_deduplication} performs hash-based, post-process duplicate detection to eliminate repeated values at the cache-line granularity and maps all duplicate tags to a single stored copy with limited overhead. Although it effectively increases cache capacity and improves average performance, it is brittle to small differences in data values. 

In contrast, Doppelgänger \cite{Doppelganger} proposes an approximate similarity-sharing technique in which multiple tags may point to a single representative data entry if pairwise element differences stay within a threshold. Rather than explicitly compare values, Bunker Cache \cite{Bunker_Cache} uses the idea that blocks at periodic addresses tend to be similar and remaps such addresses to the same cache location. However, these similarity approximation mechanisms are only feasible in error-tolerant domains such as vision or signal processing. To relax all-bit equality while remaining lossless, BCD \cite{BCD_deduplication}  stores the common high bits once and encodes the high-entropy lower bits. Relative to cache-line compression alone, it markedly increases compression while keeping performance overhead small. EPC \cite{Exploiting_Inter-block_Entropy} generalizes this direction by mining recurring low-entropy regions across many blocks and using pattern-based extraction. 

However,  neither intra-line nor inter-line compression alone can fully address the cache redundancy due to their complementary strengths. And these motivate a new thread of research that combines both to maximize effective cache capacity.

\subsection{Taking advantage of both}
As redundancy can occur within and across cache lines, recent research has explored hybrid compression mechanisms that exploit both granularities to further advance the cache capacity. Dictionary Sharing \cite{Dictionary_sharing} improves inter-line compaction by extracting distinct 4-byte chunks from neighboring blocks and storing them in a shared dictionary by exploiting both content locality and upper-bit similarity. Both 2DCC \cite{2DCC} and Thesaurus \cite{Thesaurus} simultaneously exploits inter-line deduplication and intra-line compression using a hashing and locality-sensitive hashing (LSH) accordingly, where duplicates or similar clusters are replaced with references and then unique blocks are compressed. Unlike prior designs, XOR Cache \cite{The_XOR_Cache} extends hybrid compression by storing the bitwise XOR of similar line pairs, layered on top of traditional intra-line techniques. This reduces the storage demand by half with minimal performance loss.

\begin{table*}[hbt!]
\centering
\caption{Cache Parameters}
\label{tab:para}
\begin{tabular}{lccccccc}
\toprule
\textbf{Associativity} &
\textbf{Tag Latency} &
\textbf{Data Latency}&
\textbf{Response Latency} &
\textbf{mshrs}&
\textbf{tgts\_per\_mshr}&
\textbf{Size}\\
\midrule
2 & 1 & 1 & 1 & 8 & 16 & 4KiB\\
\bottomrule
\end{tabular}
\end{table*}

\section{The XOR Cache}
The XOR Cache \cite{The_XOR_Cache} is a very recently proposed last-level cache (LLC) architecture. Rather than storing duplicate copies of same cache line in different layer of memory, XOR cache store the XORed result of two cache line. Its core idea is that, the same line that appear in L1, L2, and the LLC is wasted space. For example, if line A is already in every core’s private cache, the LLC still stores another full copy of A, even though the system doesn’t really need it. This duplication is considered overhead. Instead instead of storing A and B separately in the LLC, XOR Cache stores just $A\oplus B$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{XOR_VS_Traditional.png}
    \caption{Traditional versus XOR-based cache line storage.}
    \label{fig:XOR_VS_Traditional}
\end{figure}

Here’s the intuition as it is also shown in the figure \ref{fig:XOR_VS_Traditional}:

Suppose $A = 11110000$ already exists in an L1 cache. And the LLC wants to store $B = 11110011$.

The XOR Cache stores only the XOR:
$$A\oplus B=00000011$$

Since one copy of A would definitely exist in some L1/L2 cache , the system can always reconstruct B when needed:
$$ B=(A \oplus B) \oplus A $$

So instead of considering all those private-cache copies of A as waste, XOR Cache treats them as free information that lets it compress two lines into one LLC slot.

By pairing up similar cache lines and storing only their XOR, the design performs inter-line compression. And since XORing similar lines reduces their randomness, the resulting data becomes easier for traditional intra-line compression schemes like BDI\cite{Base-delta-immediate_compression} or BVC\cite{Base-victim_compression} to shrink even further. Eventually, this produces a much more compact and power-efficient LLC that performs nearly as fast as a regular one, while storing two cache lines in the space of one whenever XOR compression is applied.

\section{Implementation}

\subsection{Simulator Configuration}

The gem5 simulator (version ~23.1) set up in syscall-emulation (SE) mode for the X86 ISA is used to evaluate the XOR-based compression mechanism. The simulated platform operates in timing mode with a single \texttt{X86TimingSimpleCPU} clocked at 2 GHz. Main memory functions as a DDR3-1600 device connected via a system crossbar interconnect to a 256 MIB physical address space.

\subsection{System Configuration}

The system uses the L1I cache and L1D cache from gem5's classic cache hierarchy; wherein which the cache compression is done on the L1D cache. Cache parameters we defined in table\ref{tab:para}.

The cache-line size is fixed at 64\,B across the hierarchy. This default setup ensures that all observed affects on the results come solely from the compression
mechanism.

Compression support is enabled selectively using gem5's
\texttt{CompressedTags} interface. A baseline uncompressed configuration, and a zero-value compressor, an FPC-style compressor, and the proposed XOR-based compressor are tested as the baseline compression comparisons. When
compression is enabled, a maximum compression ratio of 2:1 is enforced. As aforementioned, compression only applied to the L1D, while upper-level caches
remain uncompressed.

\subsection{XOR Compressor Design}

The XOR compressor is implemented as an extension of gem5’s modular compression
framework. It inherits all the properties from the \texttt{compression::Base} class and overrides only
the \texttt{compress()} and \texttt{decompress()} mechanisms to apply a
lightweight, reversible XOR transform to each cache line. Internally, each line
is represented as a vector of fixed-size \texttt{Chunk} units.

During compression, the first chunk of the line is stored verbatim, while each
subsequent chunk is XORed with its immediate predecessor:
\[
    C_0 = X_0,\qquad
    C_i = X_i \oplus X_{i-1},\quad i \ge 1,
\]
where $X_i$ and $C_i$ denote the original and encoded chunks, respectively. This
prefix-XOR transform reduces intra-line entropy and increases compatibility with
conventional pattern-based compressors such as BDI\cite{Base-delta-immediate_compression} or BVC\cite{Base-victim_compression}. Compression latency
is modeled according to the number of chunks processed per cycle, consistent
with gem5’s existing compressor timing model.

Decompression applies the inverse transformation:
\[
    X_0 = C_0,\qquad
    X_i = C_i \oplus X_{i-1},\quad i \ge 1,
\]
reconstructing the original chunk sequence before it is written back into the
cache line. The decompression
latency used here is the same as the compression latency. In the cases where the compressed chunks are all 0, these chunks are simply removed.

\subsection{Integration with the Cache Hierarchy}

The XOR compressor can be applied to any cache structure that supports compressed tags and integrates easily with gem5's compression framework. To evaluate its effect on capacity utilization, efficient line storage, and overall memory-system performance, the L1D is set up to use the XOR compressor in the evaluation setup. In order to ensure that variances in observed behavior are solely caused by the application of XOR-based compression at the L1D, all other cache levels maintain uncompressed tag stores.

The code for the program, benchmarks, and results can be viewed here: 

\href{https://github.com/ConnorA19/XORCacheCompression}{https://github.com/ConnorA19/XORCacheCompression.}

\begin{table*}[hbt!]
\centering
\caption{Zero Compressor Against PARSEC Benchmarks}
\label{tab:zero}
\begin{tabular}{lccccccc}
\toprule
\textbf{Benchmark} &
\textbf{CPI} &
\textbf{IPC} &
\textbf{Compressions} &
\textbf{Failed Compressions} &
\textbf{Avg. Bits/Block} &
\textbf{Compressed Fraction} &
\textbf{Compression Ratio} \\
\midrule
blackscholes	&	12.062736	&	0.0829	&	978609	&	967058	&	505.9566139	&	0.988196512	&	1.011944475	\\
bodytrack	&	5.737064	&	0.174305	&	67089	&	65775	&	501.9720073	&	0.980414077	&	1.019977195	\\
facesim	&	5.740146	&	0.174212	&	68367	&	66976	&	501.5828104	&	0.979653927	&	1.020768634	\\
ferret	&	6.161986	&	0.162285	&	277303	&	234456	&	432.8891934	&	0.845486706	&	1.182750708	\\
fluidanimate	&	2.743121	&	0.364548	&	2905593	&	2674732	&	471.3195496	&	0.920545995	&	1.086311825	\\
freqmine	&	4.288534	&	0.23318	&	58130968	&	56607903	&	498.5853037	&	0.973799421	&	1.026905519	\\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[hbt!]
\centering
\caption{FPC Compressor Against PARSEC Benchmarks}
\label{tab:FPC}
\begin{tabular}{lccccccc}
\toprule
\textbf{Benchmark} &
\textbf{CPI} &
\textbf{IPC} &
\textbf{Compressions} &
\textbf{Failed Compressions} &
\textbf{Avg. Bits/Block} &
\textbf{Compressed Fraction} &
\textbf{Compression Ratio} \\
\midrule
blackscholes	& 12.10066	& 0.08264	& 523145	& 224266	& 340.7582678	& 0.665543492	& 1.502531408 \\
bodytrack	&	5.841984	&	0.171175	&	56074	&	33098	&	388.2586582	&	0.758317692	&	1.318708519	\\
facesim	&	5.821818	&	0.171768	&	57596	&	35737	&	399.843878	&	0.780945074	&	1.280499785	\\
ferret	&	6.671583	&	0.149889	&	259300	&	135472	&	326.6549634	&	0.637997975	&	1.567403093	\\
fluidanimate	&	3.359045	&	0.297704	&	2675774	&	1781639	&	393.6092301	&	0.768768027	&	1.300782504	\\
freqmine	&	4.558519	&	0.219369	&	47823890	&	22158798	&	340.1238713	&	0.664304436	&	1.505333919	\\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[hbt!]
\centering
\caption{XOR Compressor Against PARSEC Benchmarks}
\label{tab:xor}
\begin{tabular}{lccccccc}
\toprule
\textbf{Benchmark} &
\textbf{CPI} &
\textbf{IPC} &
\textbf{Compressions} &
\textbf{Failed Compressions} &
\textbf{Avg. Bits/Block} &
\textbf{Compressed Fraction} &
\textbf{Compression Ratio} \\
\midrule
blackscholes	&	12.032042	&	0.083111	&	945851	&	822876	&	473.3396698	&	0.924491543	&	1.081675661	\\
bodytrack	&	5.748977	&	0.173944	&	66964	&	60093	&	481.1803357	&	0.939805343	&	1.064050133	\\
facesim	&	5.75209	&	0.17385	&	68195	&	61923	&	483.7863186	&	0.944895154	&	1.058318477	\\
ferret	&	6.158059	&	0.162389	&	272476	&	194896	&	407.33211	&	0.795570527	&	1.256959585	\\
fluidanimate	&	2.768873	&	0.361158	&	2876217	&	2420230	&	455.1492575	&	0.888963394	&	1.124905713	\\
freqmine	&	4.196351	&	0.238302	&	54480826	&	43735756	&	440.8606075	&	0.861055874	&	1.16136482	\\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[hbt!]
\centering
\caption{Default PARSEC Benchmarks: The "None" compressor; the control tests with no compressor used, therefore no results for compressions, failed compressions, avg. bits/block, compressed fraction, and compression ratio}
\label{tab:none}
\begin{tabular}{lccccccc}
\toprule
\textbf{Benchmark} &
\textbf{CPI} &
\textbf{IPC} \\
\midrule
blackscholes	&	11.569317	&	0.086436	\\
bodytrack	&	5.06505	&	0.197431	\\
facesim	&	5.150085	&	0.194172	\\
ferret	&	5.916751	&	0.169012	\\
fluidanimate	&	2.366741	&	0.422522	\\
freqmine	&	3.960228	&	0.252511	\\
\bottomrule
\end{tabular}
\end{table*}



\section{Benchmarks \& Testing Metrics}

The PARSEC benchmark suite \cite{Parsec}, and and a number of its programs, were used as benchmarks for testing the XOR cache. These benchmarks are used across scholarship \cite{Base-delta-immediate_compression, Last-level_cache_deduplication, 2DCC} for testing other schemes of cache compression. For each of the chosen PARSEC benchmarks - blackscholes, bodytrack, facesim, ferret, fluidanimate, and freqmine - the performance of XOR cache was tested against a cache with no compression scheme, as well as with gem5's FPC and Zero compressors for comparison.

In addition to the PARSEC benchmarks, our XOR cache compression algorithm was run on a set of 4 smaller benchmark tests. These benchmarks were built for the purpose of easily viewing the functioning of the XOR cache and its compression output on a small, easily digestible scale. The pseudocode for each of these tests can be seen in Appendix A, but are briefly described below:

\begin{enumerate}
    \item find\textit{1}In\textit{N}Elements: Fill an array with all 0s, except one random single value. Then tasked with scanning array to find non-zero value.
    \item count\textit{5}sIn\textit{N}Elements: Fill an array with sequentially increasing values. Then tasked with counting the number of 5s in the array.
    \item count\textit{5}sInRandom\textit{N}Elements: Fill an array with random values (non-sequential). Then tasked with counting the number of 5s in the array.
    \item count\textit{5}sInSparse\textit{N}Elements: Fill an array with an item every next 5th item in the array. Then tasked with counting the number of 5s in the array.
\end{enumerate}

These simple tests were chosen to see how well our XOR cache compression algorithm is able to compress repetitive and reoccurring data, as well as whether or not it is able to decompress the data accurately. An accurate decompression would lead to correct results for each of the benchmarks' tasks, such as counting the number of 5s in the array.

The metrics being collected for each test are instructions per cycle (IPC), cycles per instructions (CPI), and the rate of successful compressions. These metrics were chosen in order to determine not just whether or not our XOR cache is functioning, but also whether or not the usage of the compression scheme improves overall cache performance.


\begin{table*}[!h]
\centering
\caption{Performance and compression metrics averaged across PARSEC benchmarks. 
A compression is considered \emph{failed} if the resulting compressed size exceeds 50\% of the original 512-bit cache block (i.e., $>$256 bits).}
\label{tab:compression_summary}
\begin{tabular}{lccccccc}
\toprule
\textbf{Scheme} &
\textbf{CPI} &
\textbf{IPC} &
\textbf{Compressions} &
\textbf{Failed Compressions} &
\textbf{Avg. Bits/Block} &
\textbf{Compressed Fraction} &
\textbf{Compression Ratio} \\
\midrule
Zero & 6.12 & 0.20 & 10,404,654.83 & 10,102,816.67 & 485.38 & 0.95 & 1.06 \\
XOR  & 6.11 & 0.20 & 9,785,088.17  & 7,882,629.00  & 456.94 & 0.89 & 1.12 \\
FPC  & 6.39 & 0.18 & 8,565,963.17  & 4,061,501.67  & 364.87 & 0.71 & 1.41 \\
None & 5.67 & 0.22 & N/A           & N/A           & N/A    & N/A  & N/A  \\
\bottomrule
\end{tabular}
\end{table*}

\section{Results}
In this section, we evaluate the XOR compression scheme separately on the default benchmarks and our custom-defined benchmarks, focusing on performance metrics (CPI and IPC), compression effectiveness (number of compressions and average bits per block), and compression efficiency measured by compression ratio and compressed fraction.

\subsection{Evaluation based on default benchmark}

\textbf{1. Performance Metrics (CPI and IPC)}

There is a clear trend exhibited in the results for cycles per instruction (CPI) and instructions per cycle (IPC) from our experiments. As seen in the report of average results across the PARSEC benchmark tests (Table \ref{tab:compression_summary}), on average, FPC records both the highest CPI, and the lowest IPC numbers. Whereas control test with no compression method displays the reverse results with the lowest CPI and highest IPC values. This is expected because it does not perform any compression operations and introduces no overhead. Both the Zero compressor and our XOR compression methods show results very similar to each other, with average results equidistant in between what is seen from FPC and the control tests. This indicates that XOR’s compression/decompression pipeline introduces minimal latency while still effectively reducing memory traffic.

FPC shows the weakest performance of the compressed schemes, due to its deeper encoding logic and frequent pattern checks. Zero compression performs slightly worse than XOR but better than FPC. While it provides fast operation, its effectiveness is limited when data redundancy is low.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{CPI vs Compressor Type (1).png}
    \caption{CPI by compressor type for each of four unique benchmark tests}
    \label{fig:cpisimple}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{IPC vs Compressor Type (1).png}
    \caption{IPC by compressor type for each of four unique benchmark tests}
    \label{fig:ipcsimple}
\end{figure}

When tested against the 4 unique benchmarks(figures \ref{fig:cpisimple} \& \ref{fig:ipcsimple}), similar trends were recorded. The FPC compressor displayed the worst performance with the greatest CPI and lowest IPC values for each benchmark test. Unlike with the PARSEC benchmarks, there is no significant difference between CPI and IPC values for the other 3 compressor types. This means for small scale tests such as what is done in the 4 unique benchmarks, both Zero and XOR perform similarly to the control, none compressor. The results from these benchmark tests further support that a compression scheme like the XOR cache introduces minimal, to even insignificant latency when  performing compressions.

\medskip

\textbf{2. Compression Effectiveness (Compressions and avg\_bits\_per\_block)}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{heatmap.jpg}
    \caption{Compression Size Distribution Heatmap.}
    \label{fig:heatmap}
\end{figure}

In addition to investigating the affect to latency through the inclusion of compression schemes, we also tested for the effectiveness of the compressions. The effectiveness of compression was determined by the number of successful compressions, and the average number of bits compressed per block across a number of block sizes. Block sizes range from $0$ to $2^9$ bits.

XOR achieves a relatively efficient avg\_bits\_per\_block rate. Unlike Zero, which simply checks if a block is only made up of 0s to remove it, the XOR cache converts similar chunks in a block to 0 and removes them. Therefore XOR compression consistently delivers relatively high compression counts, especially when compared to the Zero compressor, and reduces average block size across all applications due to its delta-encoding behavior. This allows XOR to reduce block sizes in a more uniform and robust manner, even when redundancy is limited. The relationship between these compressors across block sizes can be seen in figure\ref{fig:heatmap}.  

Where the XOR and Zero compressors show significant gain in the average number of bits per block at the largest block size (512-bits), FPC does not show the same drastic increase. Instead, there is only a moderate jump in bits compressed per block. Results for individual PARSEC benchmarks each compression method are included in tables \ref{tab:zero}-\ref{tab:xor}. When just looking at the amount of average bits stored per block, FPC is the preferred compressor with the greatest efficiency.

\medskip

\textbf{3. Compression Ratio and Fractional Size}
The compression ratio trends also match the previously discussed avg\_bits\_per\_block trend. As seen in table\ref{tab:compression_summary}, Zero exhibits the lowest compression ratio overall, due to its limited compression scheme that focuses only on redundant 0s in memory. Although the XOR cache ranks second, its strong compression ratios reflect high-quality compression across diverse workloads. Finally, FPC delivers the highest compression ratio overall across the PARSEC benchmarks.



\section{Discussion}
\subsection{Performance Observations}
Based on the experiments done in our investigation, the XOR cache is found to be a method of compression able to achieve notable compression of memory while retaining low latency. The XOR cache consistently returns results that suggest it is a more efficient compression method than the Zero compressor, with lower average bits per block, and a higher compression ration. However, the XOR cache is unable to outperform all methods of compression, as it remains inferior to the compression efficiency of the FPC compressor. While it is true that the XOR cache may fall a bit short when it comes to compression efficiency, it makes up for it with its significant performance metrics: CPI and IPC. Of all the compressors tested, the XOR has the best CPI and IPC values, with the lowest CPI value of the compressors at 6.11 CPI, and the highest IPC value - tied with Zero - of the compressors at 0.20 IPC. 

With the results from our investigation, it can be claimed that the XOR cache is a successful method of relatively efficient cache compression with low latency in its own right. However, one of the most notable parts about the XOR cache is its potential for improving the efficiency of other compressors. When implemented alongside other compressors, especially other inter-line focused compressors - as will be discussed \ref{lim} -  the XOR Cache can further improve the compression ratio of that method by leveraging the compressibility of the XORed data.

\subsection{Challenges}
There were a number of challenges that we experienced through the process of implementing and testing the XOR cache. One such challenge that was discovered early on was gem5 version compatibility. The most recent stable version of gem5 - as of the time this project began - was version 24.1. However, this version of gem5, as well as even more recent development branches, were not compatible with the compressors needed for building the XOR cache. It was because of this incompatibility that we were faced with a number of "bad\_function\_call" and assertion errors. It was once we switched the version to v23.1 that we were able to get around these difficulties. This is the version that our XOR cache has been implemented in.

\subsection{Limitations} \label{lim}
While the primary benefits of the XOR cache are derived from its ability to leverage repetition across and within cachelines, we were unable to completely replicate both methods of compression. Our implementation of the XOR cache focused on intra-line redundancy. Additionally, our approach to addressing XORed lines was quite naive. We chose to simply remove any XORed cachelines that would XOR as a series of 0s. While affective in replicating the intention behind the XOR cache of reducing repetitive memory through a simple and reversible compression algorithm, our implementation lacks the complexity of the original design\cite{The_XOR_Cache}.

\section{Conclusion}
Over a variety of benchmark tests, and across a number of memory block sizes, the XOR cache is able to successfully compress data with latency values near that of a default, non-compression cache. The XOR cache performs at or above the performance of other compression methods for all benchmarks tested. While our implementation of the XOR cache as a naive, intra-line only compression scheme was able to report positive results, there is greater potential for this structure than what we were able to achieve. Our implementation of the XOR cache greatly increases the amount of the same value (0) blocks, and proves just how many blocks of repetitive data exist in memory through its competitive compression rate. If used alongside a scheme that uses interline compression, the XOR cache would only result in further benefits. With low latency and notable successful compression results, the XOR compressed cache only has the potential to further improve cache performance.


\bibliographystyle{IEEEtran}
\bibliography{reference}

\section{Appendix}

\subsection{Benchmark Pseudocode}

\noindent\textbf{1) find\textit{1}In\textit{N}Elements}
\begin{tcolorbox}[myalgo]
\begin{algorithmic}

\State Allocate array $A$ of size $n$
\State $index = \text{rand()} \% n$
\State $A[index] = 1$
\State $retIndex = -1$

\For{$i = 0$ to $n-1$}
    \If{$A[i] == 1$}
        \State $retIndex = i$
    \EndIf
\EndFor

\State \Return $retIndex$

\end{algorithmic}
\end{tcolorbox}



\noindent\textbf{2) count\textit{5}sIn\textit{N}Elements}
\begin{tcolorbox}[myalgo]
\begin{algorithmic}

\State Allocate array $A$ of size $n$

\For{$i = 0$ to $n-1$}
    \State $A[i] = i \% 10$
\EndFor

\State fiveCount = 0

\For{item $x$ in $A$}
    \If{$x == 5$}
        \State fiveCount += 1
    \EndIf
\EndFor

\State \Return fiveCount

\end{algorithmic}
\end{tcolorbox}



\noindent\textbf{3) count\textit{5}sInRandom\textit{N}Elements}
\begin{tcolorbox}[myalgo]
\begin{algorithmic}

\State Allocate array $A$ of size $n$

\For{$i = 0$ to $n-1$}
    \State $A[i] = (\text{rand()} + 1) \bmod 100$
\EndFor

\State fiveCount = 0

\For{$i = 0$ to $n-1$}
    \If{$A[i] == 5$}
        \State fiveCount += 1
    \EndIf
\EndFor

\State \Return fiveCount

\end{algorithmic}
\end{tcolorbox}



\noindent\textbf{4) count\textit{5}sInSparse\textit{N}Elements}
\begin{tcolorbox}[myalgo]
\begin{algorithmic}

\State Allocate array $A$ of size $n$

\For{$i = 0$ to $n-1$}
    \If{$i \% 10 == 5$}
        \State $A[i] = i \% 10$
    \EndIf
\EndFor

\State fiveCount = 0

\For{item $x$ in $A$}
    \If{$x == 5$}
        \State fiveCount += 1
    \EndIf
\EndFor

\State \Return fiveCount

\end{algorithmic}
\end{tcolorbox}


\end{document}
